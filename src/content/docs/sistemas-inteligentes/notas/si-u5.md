---
title: Unidad 5 - Redes Neuronales
description: Fundamentos machine learning y redes neuronales
---

## Ejercicio
>Colocar 5 caracter√≠sticas que tu consideres para comprar un auto y ponerle una ponderaci√≥n de importancia. La suma de esos factores debe ser 1

### Criterios para comprar un auto üöó

| Caracter√≠stica           | Ponderaci√≥n |
| ------------------------ | ----------- |
| Precio (barato)          | 0.4         |
| Facilidad de refacciones | 0.3         |
| Modelo                   | 0.1         |
| Eficiencia               | 0.15        |
| Marca                    | 0.05        |
Elegir una calificaci√≥n m√≠nima que personalmente consideres para adquirirlo.

### Calificaci√≥n minima ‚û°Ô∏è 8

Comparar entre estos dos autos siguiendo la ponderaci√≥n que estableciste.

| x                        | Ponderaci√≥n | Versa | Mustang |
| ------------------------ | ----------- | ----- | ------- |
| Precio (barato)          | 0.4         | 1     | 0       |
| Facilidad de refacciones | 0.3         | 1     | 0       |
| Modelo                   | 0.1         | 0     | 1       |
| Eficiencia               | 0.15        | 1     | 0       |
| Marca                    | 0.05        | 1     | 1       |
| **Total**                |             | 9     | 1.5     |

----
## Redes neuronales
### Definici√≥n

### Red neuronal
>Una Red de Neuronas Artificial podr√≠a definirse como un grafo cuyos nodos est√°n constituidos por unidades de proceso id√©nticas, y que propagan informaci√≥n a trav√©s de los arcos.
>
>En este grafo se distinguen tres tipos de nodos: los de entrada, los de salida y los intermedios.

### F√≥rmula

E = X^TW


>La salida es igual a la transpuesta de la matriz de entradas multiplicando la matriz del peso

Las se√±ales E son procesadas adem√°s por una funci√≥n llamada funci√≥n de activaci√≥n o de salida ùìï, que produce la se√±al de salida de la neurona S. Dependiendo de la funci√≥n ùìï, habr√°, distintos modelos de aut√≥matas; por ejemplo:

- Lineal: S = KE con K constante.
- Umbral: S = 1 si E >= ùúÉ, S = 0 si E < ùúÉ; siendo ùúÉ el umbral constante.
- Cualquier funci√≥n: S = ùìï(I), siendo ùìï una funci√≥n cualquiera.

>ùìï  = Funci√≥n de activaci√≥n

-----
## C√©lula McCulloch-Pitts

Diapositiva 151

## Definici√≥n - Red neuronal

>[!quote] Red Neuronal
>Una red neuronal es una colecci√≥n de neuronas de McCulloch y Pitts, todas con las mismas escalas de tiempos, donde sus salidas est√°n conectadas a las entradas de otras neuronas.

Una red neuronal de c√©lulas de McCulloch-Pitts tiene la capacidad de computaci√≥n universal. 
Es decir, cualquier estructura que pueda ser programada en una computadora, puede ser modelada mediante una red de c√©lulas de McCulloch-Pitts.

### Ejercicio
>Comprobar AND


| Entrada (X) | Peso(w) | ‚àëXw                      | Umbral | Salida (S) |
| ----------- | ------- | ------------------------ | ------ | ---------- |
| 0,0         | 1       | (0)(1) = 0<br/>(0)(1) = 0 | 1      | 0          |
| 0,1         | 1       | (0)(1) = 0<br/>(1)(1) = 1 | 1      | 0          |
| 1,0         | 1       | ()                       | 1      | 0          |
| 1,1         | 1       | (1)(1) = 1<br/>(1)(1) = 1 | 1      | 1          |

------
## Perceptr√≥n

El modelo se concibi√≥ como un sistema capaz de realizar tareas de clasificaci√≥n de forma autom√°tica. 
La idea es disponer de un sistema que, a partir de un conjunto de ejemplos de clases diferentes (patrones), fuera capaz de determinar las ecuaciones de las superficies que hac√≠an de frontera de dichas clases.¬†

La arquitectura de la **==red==** es  de una estructura ==**monocapa**==, en la que hay **un conjunto de c√©lulas de entrada**, tantas como sea necesario, seg√∫n los t√©rminos del problema; y **una o varias c√©lulas de salida**. Cada una de las c√©lulas de entrada tiene conexiones con todas las c√©lulas de salida, y son estas conexiones las que determinan las superficies de discriminaci√≥n del sistema.


ùë¶^‚Ä≤=‚àë(ùëñ=1)^ùëõ  ùë•_ùëñ ùë§_ùëñ 



y=F(y^‚Ä≤,Œò)



----
## Entrenamiento supervisado

>Ejemplo con compuerta AND

Y = valor esperado

| X1     | X2     | Y      |
| ------ | ------ | ------ |
| ==-1== | ==-1== | ==-1== |
| ==-1== | ==1==  | ==-1== |
| 1      | -1     | -1     |
| 1      | 1      | 1      |
W1  = 6
W2 = 8
Œò = 2

‚àÜW1 = 1
‚àÜW2 = -1
‚àÜŒò = -1

W1 = 7
W2 = 7
Œò = 1

| Renglon | Formula    | Salida | ùìï = 1 si y>0<br/>ùìï = -1 caso contrario | Esperado<br/>ùëë(ùúí) |
| ------- | ---------- | ------ | --------------------------------------- | ------------------ |
| 1       | -7 + -7 +1 | -13    | -1                                      | -1                 |
| 2       | -7 + 7+ 1  | 1      | 1                                       | -1                 |
| 3       |            |        |                                         | -1                 |
| 4       |            |        |                                         | 1                  |

‚àÜW1 = 1
‚àÜW2 = -1
‚àÜŒò = -1

W1 = 8
W2 = -6
Œò = 0

| X1  | X2  | Y   |
| --- | --- | --- |
| -1  | -1  | -1  |
| -1  | 1   | -1  |
| 1   | -1  | -1  |
| 1   | 1   | 1   |

| Renglon | Formula (X1 x W1 + X2 x W2 + Œò) | Salida | ùìï = 1 si y>0<br/>ùìï = -1 caso contrario | Esperado<br/>ùëë(ùúí) |
| ------- | ------------------------------- | ------ | --------------------------------------- | ------------------ |
| 1       | -8+6                            | -2     | -1                                      | -1                 |
| 2       |                                 |        |                                         | -1                 |
| 3       |                                 |        |                                         | -1                 |
| 4       |                                 |        |                                         | 1                  |

### Pasos a seguir

1. Empezar con valores aleatorios para los pesos y el umbral.
2. Seleccionar un vector de entrada ùúí del conjunto de ejemplos de entrenamiento.
3. Si ùë¶‚â†ùëë(ùúí), la red da una respuesta incorrecta. Modificar ùë§_ùëñ de acuerdo con:

‚àÜùë§_ùëñ=ùëë(ùúí) ùë•_ùëñ

4. Si no se ha cumplido el criterio de finalizaci√≥n, volver a 2.

----
# ADALINE

ADALINE: Adaptive Linear Neuron.
Su estructura es casi id√©ntica a la del PERCEPTRON.

En este modelo se toman en cuenta el error existente entre la salida obtenida y la salida esperada  |ùëë^ùëù‚àíùë¶^ùëù | .

A este regla de aprendizaje se le conoce como regla Delta.
Normalmente utilizamos el error cuadr√°tico medio:
(Diapositiva 159)

Dado que necesitamos minimizar el error es decir ‚àÜùëù ùë§ùëó tendremos que hacer uso del calculo diferencial para encontrar ese m√≠nimo.


‚àÜ_p w_j=-Œ≥ \frac{(‚àÇE^p)}{(‚àÇw_j )}



‚àÜ_p w_j=Œ≥(d^p-y^p ) x_j


### Procedimiento de Aprendizaje

1. Iniciar con pesos aleatorios.
2. Introducir un patr√≥n de entrada.
3. Calcular y^p, compararla con d^p y obtener su diferencia.
4. Para todos los w encontrar ‚àÜw y ponderarla por la tasa de aprendizaje ùõæ.
5. Modificar los pesos con los deltas obtenidos en el paso 4.
6. Si no se cumple con el criterio de convergencia regresar a 2.


-----
# Machine Learning

Debemos tener datos de
- Entrenamiento
- Validaci√≥n

![Machine Learning](assets/Redes%20Neuronales/Machine%20Learning/MachineLearning-IdeaPrincipal.png)

----
# Numpy

Arreglos
Matrices
Agregar elementos (append) 
Eje 0 ‚Üí Renglones
Eje 1 ‚Üí Columnas

>Considerando `numpy` con alias `np`

Insertar elementos en un arreglo

```python
nombre_arreglo = np.insert(nombre_arreglo, posicion, elemento)
```

Buscar informaci√≥n dentro un arreglo en Numpy
Funci√≥n `where`

```python
mayores0 = np.where(miArreglo2>0)
```

Operaciones vectoriales con arreglos de Numpy
Afectan a todos los elementos del arreglo

```python
miVector+=1
```

Desviaci√≥n 

```python
desviacion = miVector.std()
```


----
# üêºPandas

Importar pandas y cargar archivo CSV

```python
import pandas as pd
datos = pd.read_csv("Ruta_CSV")
print datos
```

Para convertir de CSV a arreglo de Numpy

```python
datosNumpy = np.array(datos.values)
```

## Filtros

Imprimir fechas con temperatura promedio mayor a 20 ‚ÑÉ

```python
filtro = fechas [tempProm>20]
```

Transpuesta

```python
totalDatos = totalDatos.transpose()
```

## Data frame

```
pd.Series
```

>Nos quedamos en introducci√≥n de pandas 1

Se puede recorrer con un for

Para acceder al dato por un "apuntador" se usa la funci√≥n `iloc[<indice>]`
>Ejemplo
>`nombreArreglo.iloc[0]`

Para cambiar el tipo de datos, se puede usar la funci√≥n `dtype`

Para ordenar los datos, se usa la funci√≥n `sort`

Medidas de tendencia central (media mediana y moda)

Media ‚û°Ô∏è `.mean()`
Mediana ‚û°Ô∏è `.median()`
Moda ‚û°Ô∏è `.mode()`

Una matriz, se pueden hacer dos arreglos (uno de filas y uno de columnas)

```python
indices = ["Renglon1","Renglon2","Renglon3"]
columnas = ["Columna1","Columna2","Columna3"]

datos = [[],[],[],[],[],[],[],[],[]]

```

Tambi√©n se puede poner con un diccionario 

`.head()` ‚û°Ô∏è nos pone los primeros 5 renglones
`.tail()` ‚û°Ô∏è nos pone los √∫ltimos 5

Para insertar un rengl√≥n ‚û°Ô∏è

```python
pd.Series(name="", data=<datos>, index =["Columna1","Columna2","Columna3"] )
```

---
# Problemas de Clasificaci√≥n
## Descenso del gradiente

>[!swords] Ejemplo IBM
>[¬øQu√© es el descenso del gradiente? | IBM](https://www.ibm.com/mx-es/topics/gradient-descent)

Este m√©todo busca los mejores pesos y sesgos que minimizan la p√©rdida, y elimina los valores at√≠picos del conjunto de datos. 
El objetivo del descenso de gradiente es reducir el error o la funci√≥n de costo de un modelo, al realizar pruebas con una variable de entrada y el resultado esperado.

### Descenso¬†del gradiente por lotes  

El descenso¬†del gradiente¬†por lotes suma el error de cada punto en un conjunto de entrenamiento, actualizando el modelo solo despu√©s de que¬†se hayan evaluado¬†todos los ejemplos de entrenamiento. Este proceso se conoce como √©poca de entrenamiento.

Si bien este procesamiento por lotes proporciona eficiencia de c√°lculo, a√∫n puede requerir un tiempo de procesamiento prolongado para grandes conjuntos de datos de entrenamiento, ya que a√∫n necesita almacenar todos los datos en la memoria. El descenso del gradiente por lotes tambi√©n suele producir un gradiente de error estable y una convergencia, pero a veces ese punto de convergencia no es el m√°s ideal, ya que encuentra el m√≠nimo local frente al global.

### Descenso del¬†gradiente estoc√°stico  

El descenso¬† del gradiente estoc√°stico¬†(SGD, sigla en ingl√©s de stochastic gradient descent) ejecuta una √©poca de entrenamiento para cada ejemplo dentro del conjunto de datos y actualiza¬†los par√°metros de cada ejemplo de entrenamiento uno a la vez. Dado que solo es necesario tener un ejemplo de entrenamiento, son m√°s f√°ciles de almacenar en la memoria. Si bien estas actualizaciones frecuentes pueden ofrecer m√°s detalles y velocidad, pueden generar p√©rdidas en la eficiencia computacional en comparaci√≥n con el descenso del ¬†gradiente por lotes. Sus actualizaciones frecuentes pueden generar gradientes ruidosos, pero esto tambi√©n puede ser √∫til para escapar del m√≠nimo local y encontrar el global.

### Descenso del¬†gradiente por mini lotes  

 El descenso del¬†gradiente por mini lotes¬†combina conceptos tanto del descenso del gradiente por lotes como del descenso del gradiente estoc√°stico. Divide el¬†conjunto de datos de entrenamiento¬†en lotes peque√±os¬†y realiza actualizaciones en cada uno de esos lotes. Este enfoque logra un equilibrio entre la eficiencia computacional del descenso del¬†gradiente por lotes¬†y la velocidad del descenso del¬†gradiente estoc√°stico.

### Ejemplo MNNIST
>[El dataset MNIST | Interactive Chaos](https://interactivechaos.com/es/manual/tutorial-de-deep-learning/el-dataset-mnist)

Dataset: MNNIST

-----
## M√°quinas de vector soporte (SVM)

>[!cpu-blck] Ejemplo SVM  ‚Üí IBM
>[Funcionamiento de SVM - Documentaci√≥n de IBM](https://www.ibm.com/docs/es/spss-modeler/saas?topic=models-how-svm-works)

Puede hacer
- Clasificaciones lineales
- Clasificaciones no lineales
- Regresiones
- Detecci√≥n de valores at√≠picos

>[!caution] Se tienen que escalar y normalizar los datos

### Tipos
#### Clasificadores de margen duro
Cuando forzamos al modelo para que todos los datos de entrenamiento quedan fuera del vector soporte
>Ejemplo una calle muy angosta

#### Clasificadores de margen blando
Cuando el margen entre los vectores de soporte es buen aunque algunos datos de entrenamiento caigan en el
>Ejemplo, una avenida m√°s ancha 

### Hiperparametros de un SVM

C: distancia entre los vectores de soporte
- Un valor bajo de C nos da una distancia grande
- Un valor alto de C nos dar√° una baja distancia

----
## Pasos para crear un Modelo SVM

1. Obtener datos y clase de entrenamiento
2. Escalar los datos de entrenamiento 
	1. Normalmente se usa la funci√≥n ==StandardScaler==
3. Crear el modelo linear SVC
4. Entrenar el modelo con los datos y las clases de entrenamiento
5. Realizar predicciones con el modelo entrenado.

### StandardScaler
Normaliza todos los datos
- Establece la media en 0
- Establece la desviaci√≥n est√°ndar en 1

>[!quote] Ejemplo
>[LinearSVC ‚Äî scikit-learn 1.5.2 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)

Nos quedamos en la Diapositiva del SVM del iris (graficaci√≥n del SVM )

----
## SVM como clasificadores no lineales

Para hacer esto podemos usar un transformador `PolynomialFeatures`; posteriormente se realiza 

Ejemplo:

```python
polynomial=Pipelenie([("poly_features",PolynomialFeatures(degree=3))
("scaler",StandardScaler()),
("svm_clf",LinearSVC(C-10,loss="hinge"))
])
```

Otra forma de manejar caracter√≠sticas polinomiales es modificar el kernel del modelo de lineal a polinomial

```python
poly_kernel_svm=Pipeline([
("scaler",StandardScaler()),
("svm_clf",SVC(kernel="poly",degree=3,coef0=1,C=5))
])
poly_kernel_svm.fit(x,y)
```

Kernel de funci√≥n de base radial gaussiana

```python
rbf_kernel_svm_clf=Pipeline([
("scaler",StandardScaler()),

])
```

----
## Regresiones con SVM

Este modelo soporta regresiones lineales y no lineales
En los modelos de regresi√≥n la idea es que la mayor√≠a de los datos de entrenamiento queden localizados entre los dos vectores de soporte.
La distancia entres los vectores de soporte esta dada por un hiperparametro llamado …õ (epsilon)

----

>DataSet de Obesidad
>[Obesity Dataset](https://www.kaggle.com/datasets/suleymansulak/obesity-dataset)


-----
# √Årboles de decisi√≥n

Son un modelo de ML en donde podemos:
- Clasificaci√≥n
- Regresi√≥n

Son el componente b√°sico de los bosques aleatorios
No requieren escalado ni centrado de datos

Scikit-Learn utiliza el algoritmo `CART` para generar los √°rboles, por lo que estos solo pueden ser √°rboles binarios.

## Algoritmos
### Impureza de GINI
Un nodo es puro si gini = 0; es decir, si todas las instancias de entrenamiento de ese nodo pertenecen a la misma clase

>Por default es el algoritmo que se utiliza
### Entrop√≠a
La entrop√≠a de un nodo es 0 cuando se contienen instancias de una sola clase.

### Ejemplo
>Se usa el dataset iris

```python
from sklearn.datasets import load_iris
from sklearn.tree import DecissionTreeClassifier

iris = load_iris()
```

Para calcular las probabilidades de que una instancia pertenezca a una clase en concreto se usa el siguiente comando:

```python
tree_clf.predict_proba([[5,1.5]])
```

---
## Algoritmo CART (Classification and Regression Tree)

- Divide el conjunto de entrenamiento en dos subconjuntos mediante una sola caracter√≠stica K y un umbral t_k 
- Selecciona los par√°metros k y t_k que produzcan los conjuntos m√°s puros
- El algoritmo trabaja de manera ==recursiva== hasta completar la profundidad con la que fue configurado el modelo.
- Este algoritmo da una soluci√≥n razonablemente buena pero ==**no asegura que sea √≥ptima**== 
- La velocidad para realizar predicciones es buena ya que solo se tiene que recorre el √°rbol, el cual generalmente est√° balanceado.

Los √°rboles de decision, por naturaleza tienden a sobre-ajustar los datos (*overfitting*)

### Hiper-par√°metros
- Max depth: profundidad del √°rbol
- Min samples split n√∫mero de m√≠nimo de muestras que debe de tener el √°rbol
- Min samples leaf: n√∫mero m√°ximo de muestras de un nodo terminal
- Max leaf nodes: numero m√°ximo de nodos terminales
- Max features: n√∫mero m√°ximo de caracter√≠sticas a evaluar para dividir un nodo

----
## Regresi√≥n con √°rboles de decisi√≥n
Se usa la clase `DecissionTreeRegressor`

- El valor de la predicci√≥n es el valor medio de todas las instancias de ese nodo
- El nodo terminal tambi√©n nos arroja el erro cuadr√°tico medio.
- En el caso de la regresi√≥n, el algoritmo CART no intenta minimizar la impureza; sino lo que intenta minimizar es el ECM (error cuadr√°tico medio)

>[!caution] NO es recomendable dejarlo sin restricciones, se recomienda poner un n√∫mero m√°ximo de hojas

-----
# üå≤üé≤ Bosques aleatorios (Random Forest)

- Utilizan el concepto de "sabidur√≠a colectiva"
- Usan algoritmos de ensamblaje.
	- Se utilizar√° m√°s de un modelo ¬´un grupo de modelos¬ª
- Un grupo de predictores se denomina ensamble.
- Al ensamble de √°rboles de decisi√≥n le llamamos Random Forest.

- Logistic Regression
- SVM Classifier
- Random Forest classifier
- Other

>Un bosque aleatorio es un ensamble de puros arboles de decisi√≥n
## üó≥Ô∏è üî® Voto duro

```mermaid
graph TD
    subgraph Random Forest
        A[√Årbol 1] --> Clase_A
        B[√Årbol 2] --> Clase_B
        C[√Årbol 3] --> Clase_A
        D[√Årbol 4] --> Clase_A
        E[√Årbol 5] --> Clase_B
    end
    
    subgraph Voto Duro
        Clase_A[Clase A] -- Mayor√≠a de votos --> Predicci√≥n_Final
        Clase_B[Clase B] -- Minor√≠a de votos --> Predicci√≥n_Final
    end

```

### Ejemplo voto duro

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

log_clf = LogisticRegression()
rnd_clf = RandomForestClassifier()
svm_clf = SVC()
voting_clf = VotingClassifier()
estimators = [(lr,log_clf),('rf',rnd_clf),('svc',svm_clf)] , voting = 'hadrd'
voting_clf.fit(X_train, y_train)
```

### Evaluaci√≥n de modelos

```python
from sklear.metrics import acuracy_score
for clf in (log_clf,rnd_clf,svm_clf,voting_clf):
	clf.fit(X_train,y_train)
	y_pred = clf.predict(X_test)
	print(clf._class_._name_._,acuracy_score(y_test,y_pred))
```

## üó≥Ô∏èü™∂ Voto suave (Soft Voting)

>[!quote] Definici√≥n
>Es cuando la predicci√≥n se realiza con la probabilidad m√°s alta promedio de los clasificadores (`predict_proba()`).

>Para poder usar soft voting, tenemos que hacer que el par√°metro voting sea igual a soft: `voting = 'soft'`

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

log_clf = LogisticRegression()
rnd_clf = RandomForestClassifier()
svm_clf = SVC()
voting_clf = VotingClassifier()
estimators = [(lr,log_clf),('rf',rnd_clf),('svc',svm_clf)] , voting = 'soft'
voting_clf.fit(X_train, y_train)
```

## Bagging y pasting

Otro m√©todo para obtener un conjunto de clasificadores es el de usar el mismo algoritmo de entrenamiento con subconjuntos aleatorios distintos.

Cuando el muestreo es con reemplazo, el m√©todo se llama *bagging*, en caso contrario se llama *pasting*.

### Bagging

```mermaid
graph TD
    A[Conjunto de datos original] --> B[Muestreo con reemplazo]
    B --> C[Modelo 1]
    B --> D[Modelo 2]
    B --> E[Modelo 3]
    B --> F[Modelo n]
    C --> G[Predicciones]
    D --> G
    E --> G
    F --> G
    G --> H[Votaci√≥n/Promedio]
    H --> I[Predicci√≥n final]
```

### Pasting

```mermaid
graph TD
    A[Conjunto de datos original] --> B[Divisi√≥n en subconjuntos]
    B --> C[Modelo 1]
    B --> D[Modelo 2]
    B --> E[Modelo 3]
    B --> F[Modelo n]
    C --> G[Predicciones]
    D --> G
    E --> G
    F --> G
    G --> H[Concatenaci√≥n]
    H --> I[Predicci√≥n final]
```


>Scikit Learn puede hacer bagging y pasting usando las siguientes clases:
>- `BaggingClassifier`
>- `BaggingRegressor`
>- Para diferenciar entre bagging y pasting se tiene que establecer `bootstrap = True` para bagging y `bootstrap = False` para pasting

### Ejemplo bagging

```python
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecissionTreeClassifier
bag_clf = BaggingClassifier(DecissionTreeClassifier(),n_estimators = 500, max_samples = 100, bootstrap = True, n_jobs = -1)
bag_clf.fit(X_train,y_train)
y_pred = bag_clf.predict(X_test)
```

>## NOTA
>Si el clasificador tiene un m√©todo `predict_proba()` se realiza por default un soft voting


>En Bagging:
>- Algunas instancias se pueden muestrear varias veces para los entrenamientos de los clasificadores.
>- Algunas otras instancias pueden no muestrearse nunca, a estas instancias la llamamos *out-of-bag* `oob`
>- Podemos utilizar estas `oob` como conjunto de prueba, esto lo hacemos haciendo `oob_score = True`

### Ejemplo

```python
bag_clf = BaggingClassifier(DecisionTreeClassifier(),n_estimators=500, bootstrap=True, n_jobs-1, oob_score = True)
bag_clf.fit(X_train,y_train)
bag_clf.oob_score_

from sklearn.metrics import accuracy_score
y_pred = bag_clf.predict (X_test)
accuracy_score (y_test,y_pred)
```

## NOTA - Este modelo realiza una validaci√≥n en autom√°tico con el conjunto `oob`

### `BaggingClassifier`

- Soporta el submuestreo de caracter√≠sticas.
- Los hiper-par√°metros `max_features` y `Bootstrap_features` controlan esta caracter√≠stica.

### Ejemplo - generando Bosque Aleatorio

```python
from sklearn.ensemble import RandomForestClassifier
rnd_clf = RandomForestClassifier(n_estimators=500,max_leaf_nodes=16,n_jobs=1)
rnd_clf.fit(X_train,y_train)
y_pred_rf = rnd_clf.predict(X_test)
```

>### NOTA
>Un `RandomForestClassifier` cuenta con todos los hiper-par√°metros de un `DecissionTreeClassifier` adem√°s de todos los hiper-par√°metros de un `BaggingClassifier`

